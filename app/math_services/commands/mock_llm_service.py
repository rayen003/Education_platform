"""
Mock LLM Service for testing math commands.

This module contains a mock LLM service for testing math commands without
requiring an actual LLM service.
"""

import json
import logging
import random
from datetime import datetime
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class MockLLMService:
    """Mock LLM service for testing."""
    
    def __init__(self):
        """Initialize the mock LLM service."""
        logger.info("Initialized MockLLMService")
        
    def generate_completion(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """
        Generate a mock completion.
        
        Args:
            system_prompt: The system prompt
            user_prompt: The user prompt
            
        Returns:
            Mock completion response
        """
        logger.info(f"Mock LLM received prompt: {user_prompt[:50]} ...")
        
        # Determine the type of request based on the prompt
        if "Generate a helpful, educational hint" in system_prompt:
            return self._handle_generate_hint(system_prompt, user_prompt)
        elif "providing detailed, educational feedback" in system_prompt:
            return self._handle_generate_feedback(system_prompt, user_prompt)
        else:
            # Generic response for other types of requests
            return {"content": "This is a mock response. In production, this would be generated by an actual LLM."}
    
    def _handle_generate_hint(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """
        Generate a mock hint.
        
        Args:
            system_prompt: The system prompt
            user_prompt: The user prompt
            
        Returns:
            Mock hint response
        """
        # Extract hint count from user prompt
        hint_count = 1
        for line in user_prompt.split('\n'):
            if "Hint count:" in line:
                try:
                    hint_count = int(line.split("Hint count:")[1].strip())
                except (ValueError, IndexError):
                    pass
        
        # Extract problem type from user prompt
        problem = user_prompt.split("Problem:")[1].split("\n")[0].strip() if "Problem:" in user_prompt else ""
        
        # Generate a generic hint based on hint count
        if hint_count == 1:
            hint = "<p><strong>First Hint:</strong> Start by identifying the key variables and formulas needed for this problem.</p>"
        elif hint_count == 2:
            hint = "<p><strong>Second Hint:</strong> Apply the formula correctly, paying attention to the units and signs.</p>"
        else:
            hint = "<p><strong>Additional Hint:</strong> Double-check your calculations and make sure you've accounted for all the given information.</p>"
            
        return {"content": hint}
    
    def _handle_generate_feedback(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """
        Generate mock feedback.
        
        Args:
            system_prompt: The system prompt
            user_prompt: The user prompt
            
        Returns:
            Mock feedback response
        """
        # Determine if the answer is correct
        is_correct = "The student's answer is correct" in user_prompt
        
        # Extract attempt information
        current_attempt = 1
        max_attempts = 3
        for line in user_prompt.split('\n'):
            if "Current attempt:" in line:
                try:
                    parts = line.split("Current attempt:")[1].strip().split("of")
                    current_attempt = int(parts[0].strip())
                    max_attempts = int(parts[1].strip())
                except (ValueError, IndexError):
                    pass
        
        # Generate generic feedback based on correctness
        if is_correct:
            feedback = {
                "summary": "Good job! Your answer is correct.",
                "strengths": [
                    "You applied the correct formula",
                    "Your calculations are accurate"
                ],
                "areas_for_improvement": [],
                "next_steps": [
                    "Try more challenging problems of this type",
                    "Explore related concepts to deepen your understanding"
                ]
            }
        else:
            feedback = {
                "summary": "Your answer needs some work.",
                "strengths": [
                    "You attempted to solve the problem",
                    "You showed your work"
                ],
                "areas_for_improvement": [
                    "Review the formula being used",
                    "Check your calculations for errors"
                ],
                "next_steps": [
                    "Review the concepts related to this problem",
                    "Try solving the problem again with the hints provided"
                ]
            }
            
            # Add solution explanation if final attempt
            if current_attempt >= max_attempts:
                feedback["solution_explanation"] = "This is a placeholder for a detailed solution explanation. In production, this would be generated by an actual LLM."
        
        return {"content": json.dumps(feedback, indent=2)}
